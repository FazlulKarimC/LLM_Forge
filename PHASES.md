# LLM Research Platform - Development Phases

> Real-world AI system development roadmap

---

## Overview

| Phase | Name | Status | Key Deliverable |
|-------|------|--------|-----------------|
| 0 | Infrastructure Foundation | âœ… Complete | Project scaffold |
| 1 | Core Platform | ðŸ”² Pending | Working CRUD + API |
| 2 | Basic Inference | ðŸ”² Pending | LLM generates text |
| 3 | Evaluation & Metrics | ðŸ”² Pending | Metrics dashboard |
| 4 | Chain-of-Thought | ðŸ”² Pending | First comparison |
| 5 | RAG Pipeline | ðŸ”² Pending | Retrieval system |
| 6 | ReAct Agent | ðŸ”² Pending | Tool-using agent |
| 7 | DPO Alignment | ðŸ”² Pending | Fine-tuned model |
| 8 | Inference Optimization | ðŸ”² Pending | Performance benchmarks |
| 9 | Polish & Deployment | ðŸ”² Pending | Live demo |

---

## Phase 0: Infrastructure Foundation âœ…

**Status**: Complete

### What We Built
- [x] Project folder structure (backend/frontend/configs)
- [x] FastAPI backend scaffold with proper layering
- [x] Next.js 15 frontend with TypeScript
- [x] SQLAlchemy models (Experiment, Result, Run)
- [x] Pydantic schemas with validation
- [x] Service interfaces with TODOs
- [x] Docker Compose for Qdrant
- [x] Example experiment config

### Files Created
```
backend/app/
â”œâ”€â”€ api/         â†’ experiments.py, results.py, health.py
â”œâ”€â”€ core/        â†’ config.py, database.py
â”œâ”€â”€ models/      â†’ experiment.py, result.py, run.py
â”œâ”€â”€ schemas/     â†’ experiment.py, result.py, run.py
â””â”€â”€ services/    â†’ experiment_service.py, inference/

frontend/src/
â”œâ”€â”€ app/         â†’ Dashboard, Experiments pages
â””â”€â”€ lib/         â†’ api.ts, types.ts
```

---

## Phase 1: Core Platform

**Goal**: Make the platform functional with database operations

### Tasks
- [ ] **1.1 Database Connection**
  - Configure NeonDB connection string
  - Test async SQLAlchemy connection
  - Create database tables

- [ ] **1.2 Alembic Migrations**
  - Initialize Alembic
  - Create initial migration
  - Set up auto-migration workflow

- [ ] **1.3 Implement Experiment CRUD**
  - `create()` - Save experiment config to DB
  - `get()` - Retrieve by ID
  - `list()` - Paginated listing with filters
  - `delete()` - Soft delete

- [ ] **1.4 Frontend API Integration**
  - Connect Dashboard to real stats
  - Connect Experiments list to API
  - Connect New Experiment form
  - Handle loading/error states

- [ ] **1.5 End-to-End Test**
  - Create experiment via UI
  - Verify it appears in list
  - View experiment details

### Deliverables
- âœ… Create experiment â†’ appears in database
- âœ… List experiments with pagination
- âœ… Filter by status, method

### Technical Notes
```python
# NeonDB connection string format
DATABASE_URL=postgresql://user:pass@ep-xxx.region.neon.tech/dbname?sslmode=require
```

---

## Phase 2: Basic Inference Engine

**Goal**: Run actual LLM inference locally on GTX 1650

### Tasks
- [ ] **2.1 Model Loading**
  - Install transformers, torch
  - Load Phi-2 (2.7B) - fits in 4GB VRAM
  - Implement `load_model()` in TransformersEngine
  - Handle GPU memory errors gracefully

- [ ] **2.2 Text Generation**
  - Implement `generate()` method
  - Track input/output tokens
  - Measure latency
  - Return GenerationResult

- [ ] **2.3 Naive Prompting**
  - Format: `Question: {q}\nAnswer:`
  - Parse generated response
  - Handle edge cases (empty, too long)

- [ ] **2.4 Run Logging**
  - Save each LLM call to `runs` table
  - Log: input, output, tokens, latency
  - Associate runs with experiments

- [ ] **2.5 Execution Pipeline**
  - Implement `ExperimentService.execute()`
  - Load config â†’ Load model â†’ Run inference â†’ Save results

### Deliverables
- âœ… Hit Phi-2 with a prompt, get response
- âœ… Every call logged with metrics
- âœ… Can run experiment from UI

### Technical Notes
```python
# Phi-2 fits on GTX 1650 with float16
model = AutoModelForCausalLM.from_pretrained(
    "microsoft/phi-2",
    torch_dtype=torch.float16,
    device_map="auto",
)
```

---

## Phase 3: Evaluation & Metrics

**Goal**: Measure experiment quality with proper metrics

### Tasks
- [ ] **3.1 Dataset Loading**
  - Download TriviaQA from HuggingFace
  - Create dataset abstraction
  - Sample N examples with seed

- [ ] **3.2 Accuracy Metrics**
  - Exact string match
  - Substring containment
  - F1 token overlap score

- [ ] **3.3 Latency Metrics**
  - Collect all run latencies
  - Compute p50, p95, p99 percentiles
  - Calculate throughput (runs/second)

- [ ] **3.4 Cost Proxies**
  - Total input tokens
  - Total output tokens
  - Estimated GPU seconds

- [ ] **3.5 Results Dashboard**
  - Display metrics on experiment detail page
  - Show per-run breakdown
  - Highlight correct/incorrect examples

### Deliverables
- âœ… Accuracy: 45.2% exact match
- âœ… Latency: p50=320ms, p95=580ms
- âœ… Tokens: 15,234 input, 8,921 output

### Technical Notes
```python
# F1 score calculation
def compute_f1(prediction: str, ground_truth: str) -> float:
    pred_tokens = prediction.lower().split()
    truth_tokens = ground_truth.lower().split()
    common = set(pred_tokens) & set(truth_tokens)
    if not common:
        return 0.0
    precision = len(common) / len(pred_tokens)
    recall = len(common) / len(truth_tokens)
    return 2 * precision * recall / (precision + recall)
```

---

## Phase 4: Chain-of-Thought Reasoning

**Goal**: Implement and compare reasoning strategies

### Tasks
- [ ] **4.1 CoT Prompt Template**
  - Add "Let's think step by step" trigger
  - Format reasoning chain
  - Extract final answer

- [ ] **4.2 Answer Parsing**
  - Parse "Answer:" or "Therefore:" patterns
  - Handle multi-line reasoning
  - Fallback to last sentence

- [ ] **4.3 Ablation: Naive vs CoT**
  - Run same 100 samples with both methods
  - Same seed for reproducibility
  - Compare accuracy and latency

- [ ] **4.4 Experiment Comparison View**
  - Side-by-side metrics table
  - Highlight improvements/regressions
  - Show statistical significance

- [ ] **4.5 Document Findings**
  - Update README with results table
  - Write interpretation

### Deliverables
- âœ… First research finding documented
- âœ… Table: Naive 42% â†’ CoT 58% (+16%)
- âœ… Comparison view working

### Expected Results
| Method | Accuracy | Latency p50 | Tokens |
|--------|----------|-------------|--------|
| Naive | 42.0% | 280ms | 5,200 |
| CoT | 58.0% | 450ms | 12,800 |

---

## Phase 5: RAG Pipeline

**Goal**: Build retrieval-augmented generation system

### Tasks
- [ ] **5.1 Document Ingestion**
  - Download Wikipedia subset (10K articles)
  - Chunk into 256-token segments
  - Store with metadata (title, URL)

- [ ] **5.2 Embedding Pipeline**
  - Use `all-MiniLM-L6-v2` (384 dims)
  - Batch embed all chunks
  - Upload to Qdrant collection

- [ ] **5.3 Naive RAG**
  - Query â†’ Embed â†’ Top-5 retrieval
  - Concatenate context
  - Generate with context

- [ ] **5.4 Hybrid RAG**
  - Add BM25 sparse retrieval
  - Merge dense + sparse results
  - Deduplicate, keep top-5

- [ ] **5.5 Reranked RAG**
  - Use cross-encoder for reranking
  - Score (query, chunk) pairs
  - Select top-5 after reranking

- [ ] **5.6 Faithfulness Metric**
  - Use NLI model (BART-MNLI)
  - Check: Is answer entailed by context?
  - Compute hallucination rate

- [ ] **5.7 RAG Ablations**
  - Compare: None vs Naive vs Hybrid vs Reranked
  - Measure: Accuracy, Faithfulness, Latency

### Deliverables
- âœ… RAG pipeline with 3 variants
- âœ… Faithfulness: 0.72 â†’ 0.87 with reranking
- âœ… Finding: "Reranking reduces hallucinations by 18%"

### Architecture
```
Query â†’ [Embed] â†’ [Vector Search] â†’ [BM25 Search] â†’ [Merge] â†’ [Rerank] â†’ [Generate]
```

---

## Phase 6: ReAct Agent

**Goal**: Implement tool-using agent with traces

### Tasks
- [ ] **6.1 Tool Interface**
  - Define `Tool` base class
  - Implement: name, description, execute()
  - Return structured results

- [ ] **6.2 Implement Tools**
  - `wikipedia_search(query)` â†’ First paragraph
  - `calculator(expression)` â†’ Safe eval result
  - `retrieval(query, k)` â†’ RAG results

- [ ] **6.3 ReAct Loop**
  - Parse Thought/Action/Observation
  - Execute tool, inject observation
  - Loop until "Answer:" or max iterations

- [ ] **6.4 Agent Tracing**
  - Log full trace per run
  - Count: tool calls, failures, iterations
  - Store in JSONB column

- [ ] **6.5 Error Handling**
  - Tool execution failures
  - Parsing failures
  - Max iteration reached

- [ ] **6.6 Agent Evaluation**
  - Compare: Naive vs CoT vs RAG vs Agent
  - Measure: Accuracy, Tool Efficiency, Cost

### Deliverables
- âœ… Agent with 3 working tools
- âœ… Full traces logged
- âœ… Finding: "Agents +12% accuracy, 4Ã— cost"

### ReAct Format
```
Thought: I need to find when Paris was founded.
Action: wikipedia_search("Paris history founding")
Observation: Paris was founded in the 3rd century BC...
Thought: I now know the answer.
Answer: Paris was founded in the 3rd century BC.
```

---

## Phase 7: DPO Alignment

**Goal**: Fine-tune model with preference learning

### Environment
- **Colab T4** (16GB VRAM) - required for 7B models
- **Model**: Mistral-7B-Instruct-v0.2
- **Method**: LoRA + DPO

### Tasks
- [ ] **7.1 Preference Dataset**
  - Download Anthropic HH-RLHF
  - Format: prompt + chosen + rejected
  - Split: 800 train / 100 val / 100 test

- [ ] **7.2 Training Setup**
  - Configure DPOTrainer from `trl`
  - LoRA: rank=16, target q_proj/v_proj
  - Beta=0.1, lr=5e-7

- [ ] **7.3 Run Training**
  - Train for 1 epoch (~2 hours on T4)
  - Save checkpoints every 100 steps
  - Upload to HuggingFace Hub

- [ ] **7.4 Evaluation**
  - Helpfulness: Human preference score
  - Factuality: TriviaQA accuracy
  - Verbosity: Average response length

- [ ] **7.5 Ablations**
  - Beta: 0.05, 0.1, 0.2
  - Dataset size: 200, 500, 800

### Deliverables
- âœ… DPO-tuned model on HF Hub
- âœ… Finding: "Helpfulness +15%, Factuality -6%"
- âœ… Alignment-capability trade-off documented

### Training Config
```python
training_args = DPOConfig(
    beta=0.1,
    learning_rate=5e-7,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    num_train_epochs=1,
)
```

---

## Phase 8: Inference Optimization

**Goal**: Production-grade performance

### Tasks
- [ ] **8.1 Batching**
  - Implement `generate_batch()`
  - Test batch sizes: 1, 4, 8, 16
  - Measure throughput vs latency

- [ ] **8.2 vLLM Integration**
  - Install vLLM
  - Replace TransformersEngine for benchmarks
  - Measure PagedAttention benefits

- [ ] **8.3 Prompt Caching**
  - Detect repeated context prefixes
  - Cache and reuse embeddings
  - Measure cache hit rate

- [ ] **8.4 Full Benchmark Suite**
  - Matrix: Method Ã— Optimization
  - Collect: Accuracy, Latency, Throughput, Memory

- [ ] **8.5 Pareto Analysis**
  - Plot: Accuracy vs Latency
  - Identify Pareto frontier
  - Recommendation per use case

### Deliverables
- âœ… 4Ã— throughput with vLLM + batching
- âœ… Pareto frontier visualization
- âœ… Production recommendations

### Benchmark Matrix
| Method | Baseline | +Batching | +vLLM | +Cache |
|--------|----------|-----------|-------|--------|
| Naive | 1.0Ã— | 3.2Ã— | 4.1Ã— | 4.5Ã— |
| CoT | 1.0Ã— | 2.8Ã— | 3.5Ã— | 3.8Ã— |
| RAG | 1.0Ã— | 2.5Ã— | 3.2Ã— | 4.2Ã— |

---

## Phase 9: Polish & Deployment

**Goal**: Interview-ready portfolio piece

### Tasks
- [ ] **9.1 README Transformation**
  - Executive summary
  - Results tables with real data
  - 5+ embedded visualizations
  - Installation instructions

- [ ] **9.2 Visualizations**
  - Accuracy vs Latency scatter
  - Faithfulness comparison bar chart
  - Optimization speedup chart
  - Agent tool usage pie chart
  - Pareto frontier plot

- [ ] **9.3 Code Quality**
  - Add docstrings everywhere
  - Type hints on all functions
  - 15+ unit tests
  - Remove dead code

- [ ] **9.4 Demo Video**
  - 5-7 minute walkthrough
  - Show: Create experiment â†’ View results
  - Explain key findings

- [ ] **9.5 Deployment**
  - Frontend â†’ Vercel
  - Backend â†’ Railway
  - Database â†’ NeonDB (already)
  - Vector DB â†’ Qdrant Cloud

- [ ] **9.6 Final Documentation**
  - Limitations section
  - Future work
  - Lessons learned

### Deliverables
- âœ… Live demo URL
- âœ… README that impresses in 30 seconds
- âœ… Video walkthrough
- âœ… Clean, documented codebase

### Deployment URLs
```
Frontend: https://llm-forge.vercel.app
Backend:  https://llm-forge-api.railway.app
API Docs: https://llm-forge-api.railway.app/docs
```

---

## Interview Talking Points

### 30-Second Pitch
> "I built a config-driven LLM experimentation platform. I compared Chain-of-Thought, RAG, and ReAct agents, finding CoT improves accuracy 16% on reasoning tasks. I implemented DPO alignment and discovered a 6% factuality drop despite 15% helpfulness gains. I optimized inference with vLLM batching for 4Ã— throughput. Everything is reproducible from versioned configs."

### Key Findings Summary
| Finding | Evidence |
|---------|----------|
| CoT > Naive | +16% accuracy, +270ms latency |
| Reranking reduces hallucinations | Faithfulness 0.72 â†’ 0.87 |
| Agents are expensive | +12% accuracy, 4Ã— token cost |
| Alignment has trade-offs | +15% helpful, -6% factual |
| Batching is free perf | 3-4Ã— throughput, same accuracy |

---

## How to Use This Document

1. **Before each phase**: Review tasks, understand scope
2. **During phase**: Check off completed tasks
3. **After phase**: Update status, document findings
4. **On resume**: Find current phase, continue from there

---

*This document is the source of truth for project progress.*
